{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d36f485-e32f-48e9-ba69-51552f8697d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: task directory not found: ; skipping.\n",
      "\\begin{table}\n",
      "\\caption{All Metrics (Prompt & Index Consistency) by Task and LLM}\n",
      "\\label{tab:all_metrics}\n",
      "\\begin{tabular}{llrrr}\n",
      "\\toprule\n",
      " &  & prompt-to-line Consistency & line-to-line consistency & q&a consistency \\\\\n",
      "Task & LLM &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[t]{3}{*}{education} & Llama-3.1-8B-Instruct & 0.824 & 0.800 & 0.000 \\\\\n",
      " & gemma-2-2b-it & 0.511 & 0.928 & 0.000 \\\\\n",
      " & mistral-instruct & 0.728 & 0.975 & 0.000 \\\\\n",
      "\\cline{1-5}\n",
      "\\multirow[t]{3}{*}{therapy} & Llama-3.1-8B-Instruct & 0.164 & 0.170 & 0.000 \\\\\n",
      " & gemma-2-2b-it & 0.166 & 0.246 & 0.000 \\\\\n",
      " & mistral-instruct & 0.216 & 0.241 & 0.000 \\\\\n",
      "\\cline{1-5}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "                                 prompt-to-line Consistency  \\\n",
      "Task      LLM                                                 \n",
      "education Llama-3.1-8B-Instruct                    0.824292   \n",
      "          gemma-2-2b-it                            0.511292   \n",
      "          mistral-instruct                         0.728125   \n",
      "therapy   Llama-3.1-8B-Instruct                    0.164187   \n",
      "          gemma-2-2b-it                            0.166208   \n",
      "          mistral-instruct                         0.215686   \n",
      "\n",
      "                                 line-to-line consistency  q&a consistency  \n",
      "Task      LLM                                                               \n",
      "education Llama-3.1-8B-Instruct                  0.800343              0.0  \n",
      "          gemma-2-2b-it                          0.928150              0.0  \n",
      "          mistral-instruct                       0.974590              0.0  \n",
      "therapy   Llama-3.1-8B-Instruct                  0.170177              0.0  \n",
      "          gemma-2-2b-it                          0.245900              0.0  \n",
      "          mistral-instruct                       0.241088              0.0  \n",
      "\n",
      "LaTeX table written to all_llm_metrics.tex\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration: adjust these to your setup ---\n",
    "llms = ['Llama-3.1-8B-Instruct', 'gemma-2-2b-it', 'mistral-instruct']\n",
    "tasks = ['education', 'therapy', 'chatting']\n",
    "n_files_per_llm = 4\n",
    "\n",
    "# You can now specify a custom path for each task\n",
    "\n",
    "task_paths = {\n",
    "    \"therapy\": \"therapy/exp/05.08.25/\",\n",
    "    \"chitchat\": \"chatting/exp/04.26.25/\",\n",
    "    \"education\": \"training_data/in_education/\"\n",
    "}\n",
    "\n",
    "\n",
    "# Metric keys\n",
    "metric_keys = ['P2_prompt_consistency_score', 'P2_index_consistency_score', 'P2_q&a_consistency_score']\n",
    "rename_map = {\n",
    "    'P2_prompt_consistency_score': 'prompt-to-line Consistency',\n",
    "    'P2_index_consistency_score':   'line-to-line consistency',\n",
    "    'P2_q&a_consistency_score':     'q&a consistency'\n",
    "}\n",
    "\n",
    "records = []\n",
    "\n",
    "for task in tasks:\n",
    "    task_dir = task_paths.get(task, \"\")\n",
    "    if not os.path.isdir(task_dir):\n",
    "        print(f\"Warning: task directory not found: {task_dir}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    all_files = [f for f in os.listdir(task_dir) if f.endswith('.json')]\n",
    "    for llm in llms:\n",
    "        llm_files = sorted([f for f in all_files if llm in f])[:n_files_per_llm]\n",
    "        metric_file_means = {k: [] for k in metric_keys}\n",
    "\n",
    "        for fname in llm_files:\n",
    "            path = os.path.join(task_dir, fname)\n",
    "            try:\n",
    "                data = json.load(open(path))\n",
    "            except Exception as e:\n",
    "                print(f\"  • Couldn’t read {fname} ({e}); using zeros.\")\n",
    "                for k in metric_keys:\n",
    "                    metric_file_means[k].append(0.0)\n",
    "                continue\n",
    "\n",
    "            # handle list-of-dicts format\n",
    "            if isinstance(data, list):\n",
    "                for k in metric_keys:\n",
    "                    vals = []\n",
    "                    for i, entry in enumerate(data):\n",
    "                        v = entry.get(k, 0.0)\n",
    "                        if not isinstance(v, (int, float)):\n",
    "                            print(f\"    • Bad '{k}' in element {i} of {fname}; using 0\")\n",
    "                            v = 0.0\n",
    "                        vals.append(v)\n",
    "                    metric_file_means[k].append(np.mean(vals) if vals else 0.0)\n",
    "\n",
    "            # handle single-dict-with-list format\n",
    "            elif isinstance(data, dict):\n",
    "                for k in metric_keys:\n",
    "                    arr = data.get(k, [])\n",
    "                    if isinstance(arr, list) and len(arr) > 0:\n",
    "                        metric_file_means[k].append(np.mean(arr))\n",
    "                    else:\n",
    "                        print(f\"    • Missing/invalid '{k}' in {fname}; using 0\")\n",
    "                        metric_file_means[k].append(0.0)\n",
    "            else:\n",
    "                print(f\"    • Unexpected structure in {fname}; padding zeros.\")\n",
    "                for k in metric_keys:\n",
    "                    metric_file_means[k].append(0.0)\n",
    "\n",
    "        # pad if fewer runs\n",
    "        for k in metric_keys:\n",
    "            missing = n_files_per_llm - len(metric_file_means[k])\n",
    "            if missing > 0:\n",
    "                metric_file_means[k].extend([0.0] * missing)\n",
    "\n",
    "        rec = {'Task': task, 'LLM': llm}\n",
    "        for k in metric_keys:\n",
    "            rec[k] = np.mean(metric_file_means[k])\n",
    "        records.append(rec)\n",
    "\n",
    "# Build DataFrame with multi-index and three metric columns\n",
    "df = pd.DataFrame(records)\n",
    "table = df.set_index(['Task','LLM'])[metric_keys]\n",
    "table.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "# Export to LaTeX\n",
    "latex = table.to_latex(\n",
    "    float_format=\"%.3f\",\n",
    "    caption=\"All Metrics (Prompt & Index Consistency) by Task and LLM\",\n",
    "    label=\"tab:all_metrics\"\n",
    ")\n",
    "with open('all_llm_metrics.tex', 'w') as f:\n",
    "    f.write(latex)\n",
    "\n",
    "# Print or display\n",
    "print(latex)\n",
    "print(table)\n",
    "print(\"\\nLaTeX table written to all_llm_metrics.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3526c5c-82b2-40e3-9592-d266831c1b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain     Task         Rounds Mean     Std Dev  N   \n",
      "----------------------------------------------------------------------\n",
      "therapy    Therapy      10     0.738    0.222    100 \n",
      "therapy    Therapy      20     0.68     0.194    100 \n",
      "therapy    Therapy      40     0.638    0.186    100 \n",
      "therapy    Therapy      60     0.571    0.185    100 \n",
      "therapy    Therapy      Avg    0.657    0.207    400 \n",
      "----------------------------------------------------------------------\n",
      "chitchat   Chatting     10     0.488    0.273    100 \n",
      "chitchat   Chatting     20     0.609    0.242    100 \n",
      "chitchat   Chatting     40     0.665    0.211    100 \n",
      "chitchat   Chatting     60     0.714    0.205    100 \n",
      "chitchat   Chatting     Avg    0.619    0.249    400 \n",
      "----------------------------------------------------------------------\n",
      "teaching   Education    10     0.848    0.165    100 \n",
      "teaching   Education    20     0.798    0.137    100 \n",
      "teaching   Education    40     0.822    0.119    100 \n",
      "teaching   Education    60     0.829    0.09     100 \n",
      "teaching   Education    Avg    0.824    0.132    400 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Domain     Algorithm Rounds Mean     Std Dev  N   \n",
      "-----------------------------------------------------------------\n",
      "therapy    sft       10     0.68     0.204    10  \n",
      "therapy    sft       20     0.57     0.253    10  \n",
      "therapy    sft       40     0.525    0.368    10  \n",
      "therapy    sft       60     0.453    0.303    10  \n",
      "therapy    sft       Avg    0.557    0.3      40  \n",
      "-----------------------------------------------------------------\n",
      "therapy    ppo       10     0.76     0.25     20  \n",
      "therapy    ppo       20     0.63     0.272    20  \n",
      "therapy    ppo       40     0.685    0.241    20  \n",
      "therapy    ppo       60     0.603    0.284    20  \n",
      "therapy    ppo       Avg    0.67     0.269    80  \n",
      "-----------------------------------------------------------------\n",
      "therapy    kto       10     0.64     0.32     20  \n",
      "therapy    kto       20     0.53     0.279    20  \n",
      "therapy    kto       40     0.435    0.169    20  \n",
      "therapy    kto       60     0.3      0.272    20  \n",
      "therapy    kto       Avg    0.476    0.294    80  \n",
      "-----------------------------------------------------------------\n",
      "teaching   sft       10     0.88     0.16     10  \n",
      "teaching   sft       20     0.89     0.197    10  \n",
      "teaching   sft       40     0.98     0.046    10  \n",
      "teaching   sft       60     0.803    0.234    10  \n",
      "teaching   sft       Avg    0.888    0.185    40  \n",
      "-----------------------------------------------------------------\n",
      "teaching   ppo       10     0.86     0.237    10  \n",
      "teaching   ppo       20     0.86     0.18     10  \n",
      "teaching   ppo       40     0.735    0.314    10  \n",
      "teaching   ppo       60     0.883    0.256    10  \n",
      "teaching   ppo       Avg    0.835    0.258    40  \n",
      "-----------------------------------------------------------------\n",
      "teaching   kto       10     0.333    0.249    3   \n",
      "teaching   kto       20     0.433    0.262    3   \n",
      "teaching   kto       40     0.275    0.175    2   \n",
      "teaching   kto       60     0.117    0.05     2   \n",
      "teaching   kto       Avg    0.308    0.242    10  \n",
      "-----------------------------------------------------------------\n",
      "chitchat   sft       -      -        -        0   \n",
      "-----------------------------------------------------------------\n",
      "chitchat   ppo       10     0.98     0.06     10  \n",
      "chitchat   ppo       20     0.98     0.04     10  \n",
      "chitchat   ppo       40     0.97     0.033    10  \n",
      "chitchat   ppo       60     0.993    0.013    10  \n",
      "chitchat   ppo       Avg    0.981    0.041    40  \n",
      "-----------------------------------------------------------------\n",
      "chitchat   kto       20     0.943    0.073    7   \n",
      "chitchat   kto       40     1.0      0.0      7   \n",
      "chitchat   kto       60     0.962    0.068    7   \n",
      "chitchat   kto       Avg    0.968    0.062    21  \n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set your domain file paths here\n",
    "domain_paths = {\n",
    "    \"therapy\": \"therapy/exp/05.11.25/\",\n",
    "    \"teaching\": \"education/exp/05.06.25/\",\n",
    "    \"chitchat\": \"chatting/exp/05.06.25/\"\n",
    "}\n",
    "\n",
    "default_paths = {\n",
    "    \"therapy\": \"therapy/exp/05.08.25/\",\n",
    "    \"chitchat\": \"chatting/exp/04.26.25/\",\n",
    "    \"teaching\": \"training_data/in_education/\"\n",
    "}\n",
    "\n",
    "# Only these algorithms are considered\n",
    "algorithms = [\"sft\", \"ppo\", \"kto\"]\n",
    "\n",
    "# Results structures\n",
    "results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "default_results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "loaded_files = set()\n",
    "\n",
    "def detect_algorithm(filename):\n",
    "    name = filename.lower()\n",
    "    if \"kto\" in name:\n",
    "        return \"kto\"\n",
    "    elif \"ppo\" in name:\n",
    "        return \"ppo\"\n",
    "    elif \"sft\" in name:\n",
    "        return \"sft\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# --- Process fine-tuned (sft, ppo, kto) files ---\n",
    "for domain, base_path in domain_paths.items():\n",
    "    pattern = os.path.join(base_path, \"*.json\")\n",
    "    matched_files = glob.glob(pattern)\n",
    "\n",
    "    for file_path in matched_files:\n",
    "        file_path = os.path.abspath(file_path)\n",
    "        if any(bad in file_path.lower() for bad in [\"gemma\", \"mistral\"]):\n",
    "            continue\n",
    "        if file_path in loaded_files:\n",
    "            continue\n",
    "        loaded_files.add(file_path)\n",
    "\n",
    "        alg = detect_algorithm(file_path)\n",
    "        if alg is None:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            for entry in data:\n",
    "                if \"P2_prompt_consistency_score\" in entry and \"rounds\" in entry:\n",
    "                    score = entry[\"P2_prompt_consistency_score\"]\n",
    "                    rounds = entry[\"rounds\"]\n",
    "                    results[domain][alg][rounds].append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "# --- Process default files, excluding gemma/mistral ---\n",
    "for domain, path in default_paths.items():\n",
    "    pattern = os.path.join(path, \"*.json\")\n",
    "    matched_files = glob.glob(pattern)\n",
    "\n",
    "    for file_path in matched_files:\n",
    "        file_path = os.path.abspath(file_path)\n",
    "        if any(bad in file_path.lower() for bad in [\"gemma\", \"mistral\"]):\n",
    "            continue\n",
    "        if file_path in loaded_files:\n",
    "            continue\n",
    "        loaded_files.add(file_path)\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            for entry in data:\n",
    "                if all(k in entry for k in [\"P2_prompt_consistency_score\", \"rounds\", \"task_name\"]):\n",
    "                    score = entry[\"P2_prompt_consistency_score\"]\n",
    "                    rounds = entry[\"rounds\"]\n",
    "                    task = entry[\"task_name\"]\n",
    "                    default_results[domain][task][rounds].append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading default file {file_path}: {e}\")\n",
    "\n",
    "# --- Print default results grouped by task ---\n",
    "print(f\"{'Domain':<10} {'Task':<12} {'Rounds':<6} {'Mean':<8} {'Std Dev':<8} {'N':<4}\")\n",
    "print(\"-\" * 70)\n",
    "for domain in default_paths:\n",
    "    domain_has_data = domain in default_results and len(default_results[domain]) > 0\n",
    "\n",
    "    if not domain_has_data:\n",
    "        print(f\"{domain:<10} {'(no data)':<12} {'-':<6} {'-':<8} {'-':<8} {'0':<4}\")\n",
    "        print(\"-\" * 70)\n",
    "        continue\n",
    "\n",
    "    for task in default_results[domain]:\n",
    "        all_scores = []\n",
    "        for rounds, scores in sorted(default_results[domain][task].items()):\n",
    "            mean = round(np.mean(scores), 3)\n",
    "            std = round(np.std(scores), 3)\n",
    "            count = len(scores)\n",
    "            all_scores.extend(scores)\n",
    "            print(f\"{domain:<10} {task:<12} {rounds:<6} {mean:<8} {std:<8} {count:<4}\")\n",
    "\n",
    "        if all_scores:\n",
    "            avg_mean = round(np.mean(all_scores), 3)\n",
    "            avg_std = round(np.std(all_scores), 3)\n",
    "            total = len(all_scores)\n",
    "            print(f\"{domain:<10} {task:<12} {'Avg':<6} {avg_mean:<8} {avg_std:<8} {total:<4}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "# --- Print fine-tuned results grouped by algorithm ---\n",
    "print(f\"\\n{'Domain':<10} {'Algorithm':<9} {'Rounds':<6} {'Mean':<8} {'Std Dev':<8} {'N':<4}\")\n",
    "print(\"-\" * 65)\n",
    "for domain in domain_paths:\n",
    "    for alg in algorithms:\n",
    "        if domain not in results or alg not in results[domain]:\n",
    "            print(f\"{domain:<10} {alg:<9} {'-':<6} {'-':<8} {'-':<8} {'0':<4}\")\n",
    "            print(\"-\" * 65)\n",
    "            continue\n",
    "\n",
    "        all_scores = []\n",
    "        for rounds, scores in sorted(results[domain][alg].items()):\n",
    "            mean = round(np.mean(scores), 3)\n",
    "            std = round(np.std(scores), 3)\n",
    "            count = len(scores)\n",
    "            all_scores.extend(scores)\n",
    "            print(f\"{domain:<10} {alg:<9} {rounds:<6} {mean:<8} {std:<8} {count:<4}\")\n",
    "\n",
    "        if all_scores:\n",
    "            avg_mean = round(np.mean(all_scores), 3)\n",
    "            avg_std = round(np.std(all_scores), 3)\n",
    "            total = len(all_scores)\n",
    "            print(f\"{domain:<10} {alg:<9} {'Avg':<6} {avg_mean:<8} {avg_std:<8} {total:<4}\")\n",
    "        print(\"-\" * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0142a93c-5fe4-4be8-b60d-a72849a5fb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LLM × Task Consistency Table (LaTeX) =====\n",
      "\n",
      "\n",
      "\\begin{table*}[t]\n",
      "    \\centering\n",
      "    \\scriptsize\n",
      "    \\begin{tabular}{l l c c c}\n",
      "        \\toprule\n",
      "        \\textbf{Task} & \\textbf{LLM} \n",
      "        & \\textbf{prompt-to-line Consistency} \n",
      "        & \\textbf{line-to-line Consistency} \n",
      "        & \\textbf{Q\\&A Consistency} \\\\\n",
      "        \\midrule\n",
      "\\textit{Education} & Llama-3.1-8B-Instruct & $0.824 \\pm 0.132$ & $0.800 \\pm 0.148$ & $\\text{--}$ \\\\\n",
      " & gemma-2-2b-it & $0.511 \\pm 0.250$ & $0.928 \\pm 0.092$ & $\\text{--}$ \\\\\n",
      " & mistral-instruct & $0.728 \\pm 0.191$ & $0.975 \\pm 0.063$ & $\\text{--}$ \\\\\n",
      "\\textit{Therapy} & Llama-3.1-8B-Instruct & $0.657 \\pm 0.207$ & $0.681 \\pm 0.168$ & $\\text{--}$ \\\\\n",
      " & gemma-2-2b-it & $0.665 \\pm 0.247$ & $0.984 \\pm 0.040$ & $\\text{--}$ \\\\\n",
      " & mistral-instruct & $0.863 \\pm 0.186$ & $0.964 \\pm 0.078$ & $\\text{--}$ \\\\\n",
      "\\textit{Chatting} & Llama-3.1-8B-Instruct & $0.619 \\pm 0.249$ & $0.992 \\pm 0.025$ & $\\text{--}$ \\\\\n",
      " & gemma-2-2b-it & $0.871 \\pm 0.230$ & $0.900 \\pm 0.123$ & $\\text{--}$ \\\\\n",
      " & mistral-instruct & $0.955 \\pm 0.097$ & $0.984 \\pm 0.038$ & $\\text{--}$ \\\\\n",
      "        \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{\\textbf{LLM Consistency Metrics across Tasks.} \n",
      "    Mean and standard deviation (mean $\\pm$ std) of three consistency metrics—prompt-to-line, line-to-line, and Q\\&A consistency—for each LLM across different dialogue tasks.}\n",
      "    \\label{tab:llm_consistency}\n",
      "\\end{table*}\n",
      "\n",
      "\n",
      "===== Round Length × Task Consistency Table (LaTeX) =====\n",
      "\n",
      "\n",
      "\\begin{table*}[t]\n",
      "    \\centering\n",
      "    \\scriptsize\n",
      "    \\begin{tabular}{l c c c c}\n",
      "        \\toprule\n",
      "        \\textbf{Task} & \\textbf{Rounds} \n",
      "        & \\textbf{prompt-to-line Consistency} \n",
      "        & \\textbf{line-to-line Consistency} \n",
      "        & \\textbf{Q\\&A Consistency} \\\\\n",
      "        \\midrule\n",
      "\\textit{Education} & 10 & $0.695 \\pm 0.266$ & $0.816 \\pm 0.195$ & $\\text{--}$ \\\\\n",
      " & 20 & $0.687 \\pm 0.230$ & $0.888 \\pm 0.109$ & $\\text{--}$ \\\\\n",
      " & 40 & $0.686 \\pm 0.222$ & $0.942 \\pm 0.059$ & $\\text{--}$ \\\\\n",
      " & 60 & $0.684 \\pm 0.226$ & $0.959 \\pm 0.043$ & $\\text{--}$ \\\\\n",
      " & Avg & $0.688 \\pm 0.236$ & $0.901 \\pm 0.130$ & $\\text{--}$ \\\\\n",
      "\\textit{Therapy} & 10 & $0.732 \\pm 0.280$ & $0.775 \\pm 0.267$ & $\\text{--}$ \\\\\n",
      " & 20 & $0.726 \\pm 0.225$ & $0.872 \\pm 0.151$ & $\\text{--}$ \\\\\n",
      " & 40 & $0.713 \\pm 0.214$ & $0.909 \\pm 0.109$ & $\\text{--}$ \\\\\n",
      " & 60 & $0.706 \\pm 0.214$ & $0.926 \\pm 0.097$ & $\\text{--}$ \\\\\n",
      " & Avg & $0.719 \\pm 0.235$ & $0.871 \\pm 0.180$ & $\\text{--}$ \\\\\n",
      "\\textit{Chatting} & 10 & $0.787 \\pm 0.292$ & $0.949 \\pm 0.109$ & $\\text{--}$ \\\\\n",
      " & 20 & $0.809 \\pm 0.253$ & $0.944 \\pm 0.104$ & $\\text{--}$ \\\\\n",
      " & 40 & $0.821 \\pm 0.233$ & $0.970 \\pm 0.057$ & $\\text{--}$ \\\\\n",
      " & 60 & $0.843 \\pm 0.206$ & $0.973 \\pm 0.060$ & $\\text{--}$ \\\\\n",
      " & Avg & $0.815 \\pm 0.249$ & $0.959 \\pm 0.087$ & $\\text{--}$ \\\\\n",
      "        \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{\\textbf{Consistency Metrics across Conversation Lengths.} \n",
      "    Mean and standard deviation (mean $\\pm$ std) of each consistency metric for each task, averaged across LLMs.}\n",
      "    \\label{tab:length_consistency}\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Configuration ---\n",
    "llms = ['Llama-3.1-8B-Instruct', 'gemma-2-2b-it', 'mistral-instruct']\n",
    "tasks = ['education', 'therapy', 'chatting']\n",
    "n_files_per_llm = 4\n",
    "\n",
    "task_paths = {\n",
    "    \"therapy\": \"therapy/exp/05.08.25/\",\n",
    "    \"chatting\": \"chatting/exp/04.26.25/\",\n",
    "    \"education\": \"training_data/in_education/\"\n",
    "}\n",
    "\n",
    "metrics_to_summarize = {\n",
    "    'P2_prompt_consistency_score': 'Prompt Consistency',\n",
    "    'P2_index_consistency_score': 'Index Consistency',\n",
    "    'P2_q&a_consistency_score': 'Q&A Consistency'\n",
    "}\n",
    "\n",
    "# --- Generate LLM × Task × Metric LaTeX Table (mean ± std) ---\n",
    "latex_llm_rows = []\n",
    "for task in tasks:\n",
    "    task_dir = task_paths.get(task, \"\")\n",
    "    if not os.path.isdir(task_dir): continue\n",
    "\n",
    "    for llm in llms:\n",
    "        scores_by_metric = {k: [] for k in metrics_to_summarize}\n",
    "        all_files = [f for f in os.listdir(task_dir) if f.endswith('.json') and llm in f][:n_files_per_llm]\n",
    "\n",
    "        for fname in all_files:\n",
    "            try:\n",
    "                with open(os.path.join(task_dir, fname)) as f:\n",
    "                    data = json.load(f)\n",
    "            except:\n",
    "                continue\n",
    "            if not isinstance(data, list): continue\n",
    "\n",
    "            for entry in data:\n",
    "                for key in metrics_to_summarize:\n",
    "                    val = entry.get(key)\n",
    "                    if isinstance(val, (int, float)):\n",
    "                        scores_by_metric[key].append(val)\n",
    "\n",
    "        row = [f\"\\\\textit{{{task.title()}}}\" if llm == llms[0] else \"\", llm]\n",
    "        for key in metrics_to_summarize:\n",
    "            vals = scores_by_metric[key]\n",
    "            if vals:\n",
    "                mean = np.mean(vals)\n",
    "                std = np.std(vals)\n",
    "                row.append(f\"${mean:.3f} \\\\pm {std:.3f}$\")\n",
    "            else:\n",
    "                row.append(\"$\\\\text{--}$\")\n",
    "        latex_llm_rows.append(\" & \".join(row) + \" \\\\\\\\\")\n",
    "\n",
    "latex_llm_table = r\"\"\"\n",
    "\\begin{table*}[t]\n",
    "    \\centering\n",
    "    \\scriptsize\n",
    "    \\begin{tabular}{l l c c c}\n",
    "        \\toprule\n",
    "        \\textbf{Task} & \\textbf{LLM} \n",
    "        & \\textbf{prompt-to-line Consistency} \n",
    "        & \\textbf{line-to-line Consistency} \n",
    "        & \\textbf{Q\\&A Consistency} \\\\\n",
    "        \\midrule\n",
    "\"\"\" + \"\\n\".join(latex_llm_rows) + r\"\"\"\n",
    "        \\bottomrule\n",
    "    \\end{tabular}\n",
    "    \\caption{\\textbf{LLM Consistency Metrics across Tasks.} \n",
    "    Mean and standard deviation (mean $\\pm$ std) of three consistency metrics—prompt-to-line, line-to-line, and Q\\&A consistency—for each LLM across different dialogue tasks.}\n",
    "    \\label{tab:llm_consistency}\n",
    "\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "with open(\"llm_consistency_metrics.tex\", \"w\") as f:\n",
    "    f.write(latex_llm_table)\n",
    "\n",
    "# --- Generate Conversation Length × Task × Metric LaTeX Table (mean ± std) ---\n",
    "latex_round_rows = []\n",
    "round_lengths = [10, 20, 40, 60]\n",
    "\n",
    "for task in tasks:\n",
    "    task_dir = task_paths.get(task, \"\")\n",
    "    if not os.path.isdir(task_dir): continue\n",
    "\n",
    "    for round_len in round_lengths + [\"Avg\"]:\n",
    "        scores_by_metric = {k: [] for k in metrics_to_summarize}\n",
    "        for llm in llms:\n",
    "            files = [f for f in os.listdir(task_dir) if f.endswith('.json') and llm in f][:n_files_per_llm]\n",
    "            for fname in files:\n",
    "                try:\n",
    "                    with open(os.path.join(task_dir, fname)) as f:\n",
    "                        data = json.load(f)\n",
    "                except:\n",
    "                    continue\n",
    "                if not isinstance(data, list): continue\n",
    "\n",
    "                for entry in data:\n",
    "                    if round_len != \"Avg\" and entry.get(\"rounds\") != round_len:\n",
    "                        continue\n",
    "                    for key in metrics_to_summarize:\n",
    "                        val = entry.get(key)\n",
    "                        if isinstance(val, (int, float)):\n",
    "                            scores_by_metric[key].append(val)\n",
    "\n",
    "        row = [f\"\\\\textit{{{task.title()}}}\" if round_len == round_lengths[0] else \"\", str(round_len)]\n",
    "        for key in metrics_to_summarize:\n",
    "            vals = scores_by_metric[key]\n",
    "            if vals:\n",
    "                mean = np.mean(vals)\n",
    "                std = np.std(vals)\n",
    "                row.append(f\"${mean:.3f} \\\\pm {std:.3f}$\")\n",
    "            else:\n",
    "                row.append(\"$\\\\text{--}$\")\n",
    "        latex_round_rows.append(\" & \".join(row) + \" \\\\\\\\\")\n",
    "\n",
    "latex_round_table = r\"\"\"\n",
    "\\begin{table*}[t]\n",
    "    \\centering\n",
    "    \\scriptsize\n",
    "    \\begin{tabular}{l c c c c}\n",
    "        \\toprule\n",
    "        \\textbf{Task} & \\textbf{Rounds} \n",
    "        & \\textbf{prompt-to-line Consistency} \n",
    "        & \\textbf{line-to-line Consistency} \n",
    "        & \\textbf{Q\\&A Consistency} \\\\\n",
    "        \\midrule\n",
    "\"\"\" + \"\\n\".join(latex_round_rows) + r\"\"\"\n",
    "        \\bottomrule\n",
    "    \\end{tabular}\n",
    "    \\caption{\\textbf{Consistency Metrics across Conversation Lengths.} \n",
    "    Mean and standard deviation (mean $\\pm$ std) of each consistency metric for each task, averaged across LLMs.}\n",
    "    \\label{tab:length_consistency}\n",
    "\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "with open(\"round_length_consistency_metrics.tex\", \"w\") as f:\n",
    "    f.write(latex_round_table)\n",
    "\n",
    "print(\"\\n===== LLM × Task Consistency Table (LaTeX) =====\\n\")\n",
    "print(latex_llm_table)\n",
    "\n",
    "print(\"\\n===== Round Length × Task Consistency Table (LaTeX) =====\\n\")\n",
    "print(latex_round_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532882c8-84a4-498f-8065-57aba5e706e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
